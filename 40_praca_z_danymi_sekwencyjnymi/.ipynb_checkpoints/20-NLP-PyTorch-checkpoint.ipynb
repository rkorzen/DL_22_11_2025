{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f811ab",
   "metadata": {},
   "source": [
    "# Mini-projekt NLP w PyTorch\n",
    "Ten notatnik pokazuje najważniejsze koncepcje związane z przetwarzaniem języka naturalnego (NLP) w PyTorch na bardzo małym, sztucznym zbiorze danych. Przechodzimy przez cały proces: od przygotowania tekstu i budowy słownika, przez kodowanie sekwencji, aż po trenowanie prostej sieci RNN i wykonywanie prognoz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36be30",
   "metadata": {},
   "source": [
    "## 1. Importy i ustawienia\n",
    "Zaczynamy od zaimportowania PyTorcha i kilku pomocniczych bibliotek. Ustawiamy także ziarno losowe, aby wyniki były powtarzalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53331f",
   "metadata": {},
   "source": [
    "## 2. Mini zbiór danych\n",
    "Dla przejrzystości wykorzystamy niewielki, ręcznie przygotowany zbiór zdań oznaczonych etykietami `1` (pozytywne) lub `0` (negatywne). Dzięki temu możemy szybko podejrzeć cały zbiór i łatwiej zrozumieć każdy krok przygotowania danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    (\"kocham ten film\", 1),\n",
    "    (\"uwielbiam tę książkę\", 1),\n",
    "    (\"ten kurs jest świetny\", 1),\n",
    "    (\"co za wspaniały dzień\", 1),\n",
    "    (\"jestem zachwycony\", 1),\n",
    "    (\"nienawidzę tej gry\", 0),\n",
    "    (\"to był stracony czas\", 0),\n",
    "    (\"film był okropny\", 0),\n",
    "    (\"jestem rozczarowany\", 0),\n",
    "    (\"fatalna obsługa\", 0)\n",
    "]\n",
    "\n",
    "for text, label in samples:\n",
    "    print(f\"{label} -> {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057040c",
   "metadata": {},
   "source": [
    "## 3. Budowa słownika (vocabulary)\n",
    "Modelom NLP trudno pracować bezpośrednio na surowych słowach. Potrzebujemy mapowania słów na liczby całkowite. Budujemy słownik zawierający:\n",
    "\n",
    "- `PAD` – specjalny token wypełniający (używany przy wyrównywaniu długości sekwencji),\n",
    "- `UNK` – token dla słów nieznanych (gdyby pojawiły się inne słowa w nowych zdaniach),\n",
    "- wszystkie słowa z naszego mini zbioru.\n",
    "\n",
    "Tokenizacja będzie bardzo prosta – rozdzielamy słowa po spacjach i zamieniamy na małe litery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "counter = Counter()\n",
    "for text, _ in samples:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in counter:\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(\"Rozmiar słownika:\", len(vocab))\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0f002",
   "metadata": {},
   "source": [
    "## 4. Kodowanie i wyrównywanie sekwencji\n",
    "Każde zdanie zamieniamy na listę indeksów słów. Następnie wyrównujemy długość sekwencji do stałej wartości `MAX_LEN`, dopełniając krótsze zdania tokenem `<PAD>`. Dzięki temu możemy umieścić dane w jednej macierzy tensora.\n",
    "\n",
    "W razie napotkania nieznanego słowa skorzystamy z `UNK`, choć w tym przykładzie wszystkie słowa są znane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70563699",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 5\n",
    "\n",
    "def encode(text, vocab, max_len):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "encoded_texts = [encode(text, vocab, MAX_LEN) for text, _ in samples]\n",
    "labels = [label for _, label in samples]\n",
    "\n",
    "print(\"Przykładowa sekwencja:\", encoded_texts[0], \"=\", [inverse_vocab[idx] for idx in encoded_texts[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925a1ba",
   "metadata": {},
   "source": [
    "## 5. Dataset i DataLoader\n",
    "Korzystamy ze standardowej struktury PyTorch: tworzymy klasę `Dataset`, która zwraca pary `(tensor_wejściowy, etykieta)`. Następnie pakujemy dane w `DataLoader`, aby łatwo iterować po mini-batchach podczas treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded = torch.tensor(encoded_texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded[idx], self.labels[idx]\n",
    "\n",
    "full_dataset = SentimentDataset(encoded_texts, labels)\n",
    "\n",
    "# Prosty podział: 8 przykładów treningowych, 2 testowe\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [8, 2], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "print('Rozmiar zbioru treningowego:', len(train_dataset))\n",
    "print('Rozmiar zbioru testowego:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee68d1",
   "metadata": {},
   "source": [
    "## 6. Model RNN\n",
    "Model składa się z trzech elementów:\n",
    "1. `nn.Embedding` – zamienia indeksy słów na gęste wektory (embeddingi).\n",
    "2. `nn.RNN` – przetwarza sekwencję i zwraca stany ukryte.\n",
    "3. `nn.Linear` + `nn.Sigmoid` – mapuje ostatni stan ukryty na prawdopodobieństwo klasy `1`.\n",
    "\n",
    "Utrzymujemy model możliwie prosty, aby skupić się na przepływie danych i interpretacji wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # hidden ma kształt (num_layers, batch, hidden_size)\n",
    "        last_hidden = hidden[-1]\n",
    "        logits = self.fc(last_hidden)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze(1)\n",
    "\n",
    "model = SimpleSentimentRNN(vocab_size=len(vocab))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad69c63",
   "metadata": {},
   "source": [
    "## 7. Trening\n",
    "Używamy binarnej funkcji straty (`BCELoss`) oraz optymalizatora Adam. Ponieważ danych jest mało, wystarczy kilkanaście epok, aby model nauczył się prostych reguł. W każdej epoce wypisujemy stratę i dokładność na zbiorze treningowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c596045",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total\n",
    "    acc = correct / total\n",
    "    print(f\"Epoka {epoch:02d} | Strata: {avg_loss:.4f} | Dokładność: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aeb23",
   "metadata": {},
   "source": [
    "## 8. Ewaluacja na zbiorze testowym\n",
    "Sprawdzamy, jak model radzi sobie na dwóch przykładach testowych, które nie brały udziału w treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90475e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_correct += (preds == targets).sum().item()\n",
    "        test_total += targets.size(0)\n",
    "\n",
    "print(f'Dokładność na teście: {test_correct / test_total:.2f} ({test_correct}/{test_total})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed40e44",
   "metadata": {},
   "source": [
    "## 9. Predykcje dla nowych zdań\n",
    "Przygotowujemy pomocniczą funkcję, która przyjmuje tekst, koduje go tak jak wcześniej, a następnie zwraca prawdopodobieństwo klasy pozytywnej. Dzięki temu możemy szybko sprawdzić, jak model reaguje na nowe wypowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    encoded = torch.tensor([encode(text, vocab, MAX_LEN)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        prob = model(encoded).item()\n",
    "    return prob\n",
    "\n",
    "examples = [\n",
    "    \"kocham tę grę\",\n",
    "    \"co za fatalny dzień\",\n",
    "    \"jestem bardzo zadowolony\",\n",
    "    \"strasznie nudny film\"\n",
    "]\n",
    "\n",
    "for sentence in examples:\n",
    "    prob = predict_sentiment(model, sentence)\n",
    "    print(f\"{sentence!r} -> prawdopodobieństwo klasy pozytywnej: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4445d",
   "metadata": {},
   "source": [
    "## 10. Podsumowanie\n",
    "W tym mini-projekcie omówiliśmy podstawowy przepływ pracy w NLP z użyciem PyTorcha:\n",
    "\n",
    "1. **Przygotowanie danych** – tokenizacja, budowa słownika, kodowanie i wyrównanie sekwencji.\n",
    "2. **Struktura danych w PyTorch** – `Dataset` i `DataLoader` ułatwiają iterację po mini-batchach.\n",
    "3. **Model** – `Embedding` + `RNN` + warstwa gęsta generująca prawdopodobieństwo klasy.\n",
    "4. **Trening i ewaluacja** – klasyczna pętla treningowa z funkcją straty `BCELoss` oraz pomiarem dokładności.\n",
    "5. **Predykcje** – prosty interfejs do oceniania nowych zdań.\n",
    "\n",
    "Chociaż przykład jest niewielki, przedstawia wszystkie najważniejsze elementy pipeline'u NLP. Przy większych projektach wystarczy wymienić zbiór danych, rozszerzyć preprocessing (np. usuwanie znaków, stemming), zastosować bogatszą architekturę (GRU/LSTM/Transformer) i zwiększyć rozmiar embeddingów."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}