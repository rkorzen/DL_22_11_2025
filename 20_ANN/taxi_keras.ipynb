{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19205a7-0845-42cc-825e-edf036d35401",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Konwersja danych wejściowych z tensora PyTorch/TF na numpy arrays \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# i rzutowanie na odpowiednie typy (int32 dla kategorii, float32 dla liczb zmiennoprzecinkowych).\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Dzięki temu model w Kerasie dostaje dane w formacie, którego oczekuje.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cat_train_np = \u001b[43mcat_train\u001b[49m.numpy().astype(\u001b[33m'\u001b[39m\u001b[33mint32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m cat_test_np = cat_test.numpy().astype(\u001b[33m'\u001b[39m\u001b[33mint32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m con_train_np = con_train.numpy().astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cat_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ustawiamy ziarno losowe (seed), żeby wyniki były powtarzalne\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Konwersja danych wejściowych z tensora PyTorch/TF na numpy arrays \n",
    "# i rzutowanie na odpowiednie typy (int32 dla kategorii, float32 dla liczb zmiennoprzecinkowych).\n",
    "# Dzięki temu model w Kerasie dostaje dane w formacie, którego oczekuje.\n",
    "cat_train_np = cat_train.numpy().astype('int32')\n",
    "cat_test_np = cat_test.numpy().astype('int32')\n",
    "con_train_np = con_train.numpy().astype('float32')\n",
    "con_test_np = con_test.numpy().astype('float32')\n",
    "y_train_np = y_train.numpy().astype('float32')\n",
    "y_test_np = y_test.numpy().astype('float32')\n",
    "\n",
    "# Przygotowanie słownika wejść dla modelu:\n",
    "# - każdy atrybut kategoryczny dostaje osobny \"wektor kolumnowy\"\n",
    "# - dane ciągłe grupujemy razem w jednej macierzy pod kluczem 'cont'\n",
    "train_inputs = {f'cat_{i}': cat_train_np[:, i:i+1] for i in range(cat_train_np.shape[1])}\n",
    "test_inputs = {f'cat_{i}': cat_test_np[:, i:i+1] for i in range(cat_test_np.shape[1])}\n",
    "train_inputs['cont'] = con_train_np\n",
    "test_inputs['cont'] = con_test_np\n",
    "\n",
    "# Informacyjnie wypisujemy wersję TensorFlow (pomocne przy debugowaniu i odtwarzaniu wyników)\n",
    "\n",
    "inputs_cat = []     # lista wejść dla zmiennych kategorycznych\n",
    "embeddings = []     # lista embeddingów odpowiadających tym wejściom\n",
    "\n",
    "# Tworzymy osobne wejście i warstwę embedding dla każdej cechy kategorycznej\n",
    "for idx, (cardinality, emb_dim) in enumerate(emb_szs):\n",
    "    # Wejście (pojedyncza liczba całkowita reprezentująca kategorię)\n",
    "    input_layer = keras.Input(shape=(1,), name=f'cat_{idx}')\n",
    "    \n",
    "    # Warstwa embedding: zamiana indeksu kategorii na gęsty wektor o wymiarze emb_dim\n",
    "    embed = keras.layers.Embedding(cardinality, emb_dim, name=f'emb_{idx}')(input_layer)\n",
    "    \n",
    "    # Reshape: spłaszczenie (bo Embedding zwraca kształt (batch, 1, emb_dim))\n",
    "    embed = keras.layers.Reshape((emb_dim,), name=f'emb_{idx}_reshape')(embed)\n",
    "    \n",
    "    # Zbieramy do list\n",
    "    embeddings.append(embed)\n",
    "    inputs_cat.append(input_layer)\n",
    "\n",
    "embeddings, inputs_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b003054-1ab0-43fb-86d7-f02bbe0e6154",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# łączymy je w jeden wektor cech\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cat_features = keras.layers.Concatenate(name=\u001b[33m'\u001b[39m\u001b[33mcats_concat\u001b[39m\u001b[33m'\u001b[39m)(\u001b[43membeddings\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Dropout dla embeddingów — redukuje przeuczenie\u001b[39;00m\n\u001b[32m      6\u001b[39m cat_features = keras.layers.Dropout(\u001b[32m0.4\u001b[39m, name=\u001b[33m'\u001b[39m\u001b[33mcats_dropout\u001b[39m\u001b[33m'\u001b[39m)(cat_features)\n",
      "\u001b[31mNameError\u001b[39m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# łączymy je w jeden wektor cech\n",
    "cat_features = keras.layers.Concatenate(name='cats_concat')(embeddings)\n",
    "\n",
    "\n",
    "# Dropout dla embeddingów — redukuje przeuczenie\n",
    "cat_features = keras.layers.Dropout(0.4, name='cats_dropout')(cat_features)\n",
    "\n",
    "# Wejście dla zmiennych ciągłych\n",
    "cont_input = keras.Input(shape=(con_train_np.shape[1],), name='cont')\n",
    "\n",
    "# BatchNormalization — normalizacja danych ciągłych (przyspiesza uczenie)\n",
    "cont_features = keras.layers.BatchNormalization(name='cont_bn')(cont_input)\n",
    "\n",
    "# Połączenie cech kategorycznych i ciągłych w jeden wektor\n",
    "x = keras.layers.Concatenate(name='features_concat')([cat_features, cont_features])\n",
    "\n",
    "# Klasyczne gęste warstwy ukryte (MLP)\n",
    "for i, units in enumerate([200, 100]):\n",
    "    # Gęsta warstwa w pełni połączona z ReLU\n",
    "    x = keras.layers.Dense(units, activation='relu', name=f'dense_{i}')(x)\n",
    "    # Normalizacja batchowa stabilizuje rozkład aktywacji\n",
    "    x = keras.layers.BatchNormalization(name=f'bn_{i}')(x)\n",
    "    # Dropout dla redukcji przeuczenia\n",
    "    x = keras.layers.Dropout(0.4, name=f'dropout_{i}')(x)\n",
    "\n",
    "# Warstwa wyjściowa — 1 neuron (predykcja ceny przejazdu)\n",
    "output = keras.layers.Dense(1, name='fare')(x)\n",
    "\n",
    "# Składamy cały model — wejścia to embeddingi + cechy ciągłe\n",
    "tf_model = keras.Model(inputs=inputs_cat + [cont_input], outputs=output, name='taxi_fare_model_tf')\n",
    "\n",
    "# Kompilacja modelu:\n",
    "# - optimizer Adam (0.001)\n",
    "# - funkcja straty: MSE (błąd średniokwadratowy)\n",
    "# - metryka: RMSE (bardziej interpretowalna w jednostkach ceny)\n",
    "tf_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    loss='mse',\n",
    "    metrics=[keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    ")\n",
    "\n",
    "# Podsumowanie modelu (lista warstw, parametry, kształty)\n",
    "tf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51b213-6452-4f35-b3c3-754630fcef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
